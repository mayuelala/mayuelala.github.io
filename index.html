<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<link rel="shortcut icon" href="myIcon.ico">
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />

<meta name="keywords" content="Yue Ma, Ma Yue, THU, Tsinghua University"> 
<meta name="description" content="MaYue's Home">
<meta name="google-site-verification" content="yy_3iiS_X6pJdegdwitJMrH0LRLHXwpjrV9RKLXxKjg" />
<meta name="google-site-verification" content="yy_3iiS_X6pJdegdwitJMrH0LRLHXwpjrV9RKLXxKjg" />
<link rel="stylesheet" href="jemdoc.css" type="text/css">
<title>Yue Ma@HKUST</title>
<style>
    .smaller-image {
      width: 20%;
    }
</style>


</head>
<body>
<div id="layout-content" style="margin-top:25px">
 <a href="https://github.com/mayuelala" class="github-corner"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#FD6C6C; color:#fff; position: absolute; top: 0; border: 0; right: 0;"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>

<table>
	<tbody>
		<tr>
			<td width="670">
				<div id="toptitle">					
					<h1>Yue (Jack) Ma </h1>
					<h1></h1>
					<img src="indexpics/sign.png" alt="‰Ω†ÁöÑÂõæÁâá"  class="smaller-image">
					</div>
				<h3>üéì Phd Student</h3>
				<p>
					The Hong Kong University of Science and Technology <br>
					Hong Kong.<br>
					<br>
					Email: mayuefighting [at] gmail.com<br>
					
				</p>
				<p> 
					<a href="https://www.linkedin.com/in/yue-ma-a991b425a/"><img src="./pic/LinkedIn.png" height="30px" style="margin-bottom:-3px"></a>
					<a href="https://scholar.google.com/citations?user=kwBR1ygAAAAJ&hl=zh-CN"><img src="./pic/google_scholar.png" height="30px" style="margin-bottom:-3px"></a>
					<a href="https://github.com/mayuelala"><img src="./pic/github_s.jpg" height="30px" style="margin-bottom:-3px"></a>
					<a href="https://space.bilibili.com/358448816?spm_id_from=333.1007.0.0"><img src="./pic/bilibili.jpg" height="30px" style="margin-bottom:-3px"></a>
					<a href="pic/my_wechat.jpg"><img src="./pic/wechat.png" height="30px" style="margin-bottom:-3px"></a>
					<a href="https://www.xiaohongshu.com/user/profile/6263b02c00000000210242e0"><img src="./pic/xiaohongshu.png" height="30px" style="margin-bottom:-3px"></a>
				</p>
			</td>
			<td>
				<img src="./pic/my3.jpg" border="0" width="240"><br>
			</td>
		</tr><tr>
	</tr></tbody>
</table>

<h2>Biography</h2>
<p>
	Hello! I am a second-year Phd student of CSE at <a href="https://hkust.edu.hk/">The Hong Kong University of Science and Technology (HKUST)</a>, under the supervision of <a href="https://cqf.io/" target="_blank">Prof. Qifeng Chen</a>. 
	I obtained my M.S. in Computer Science at <a href="https://www.tsinghua.edu.cn/en/index.htm">Tsinghua University</a> in 2024, supervised by Prof. Xiu Li 
	and B.Eng in Computer Science at <a href="https://www.tyut.edu.cn/">Taiyuan University of Technology</a> in 2024. 
</p>
<p>	
	I am currently a Research Scientist Intern at Meta. Previously, I studied in the <a href="http://mmlab.siat.ac.cn/">MMLab@SIAT</a> led by Prof. <a href="https://scholar.google.com/citations?user=gFtI-8QAAAAJ&hl=en" target="_blank">Dr. Yu Qiao</a></a>.
	I was also fortunate to be an internship at Baidu, Tencent AI Lab and <a href="https://hunyuan.tencent.com//">Tencent hunyuan</a>.
</p>

<p>My research interests lie in the intersection of <b>Computer Vision</b> and  <b>Machine Learning</b>. From 2021, I started to do some research on video understanding and self-supervised learning. Now, 
	I focus on designing novel applications for image/video generation(üë™ Follow Family), world model, and other downstream AIGC tasks.</p>

<p>I am also a blogger and vlogger with over 10k followers at Rednote üç†<a href="https://www.xiaohongshu.com/user/profile/6263b02c00000000210242e0" style="color: red; display: inline;"> @È©¨Ë∑É Yue Ma</a>.</p>

<p><i style="color: red; display: inline;">Feel free to contact me by email if you are interested in discussing or collaborating with me.</i></p>
<p><i style="color: red; display: inline;">Ê¨¢ËøéÊú¨ÁßëÁîüËÅîÁ≥ªÁßëÁ†îÂêà‰ΩúÔºÅ</i></p>


<h2>News</h2>
<div style="height: 200px; overflow: auto;">
<ul>
	<li>
		[09/2025] One paper are accepted by <a href="https://link.springer.com/journal/11263">IJCV 2025</a> !  
	</li>
	<li>
		[04/2025] One paper are accepted by <a href="https://aaai.org/conference/aaai/aaai-25/">ACM MM 2025</a> !  
	</li>
	<li>
		[03/2025] Two paper are accepted by <a href="https://aaai.org/conference/aaai/aaai-25/">ICCV 2025</a> !  
	</li>
	<li>
		[01/2025] One paper are accepted by <a href="https://aaai.org/conference/aaai/aaai-25/">CVPR 2025</a> !  
	</li>
	<li>
		[12/2024] Two paper are accepted by <a href="https://aaai.org/conference/aaai/aaai-25/">ICLR 2025</a> !  
	</li>
	<li>
		[12/2024] <i style="color: red; display: inline;">Five</i> paper are accepted by <a href="https://aaai.org/conference/aaai/aaai-25/">AAAI 2025</a> !  
	</li>
	<li>
		[09/2024] One paper are accepted by <a href="https://aaai.org/aaai-conference/">WACV 2025</a> !  
	</li>
	<li>
		[08/2024] One paper are accepted by <a href="https://aaai.org/aaai-conference/">NeurIPS 2024</a> !  
	</li>
	<li>
		[07/2024] One paper are accepted by <a href="https://aaai.org/aaai-conference/">Siggraph Asia 2024</a> !  
	</li>
	<li>
		[07/2024] One paper are accepted by <a href="https://aaai.org/aaai-conference/">ACM MM 2024</a> !  
	</li>
	<li>
		[06/2024] Excited to release <i style="color: red; display: inline;">Follow-Your-Emoji</i>, a freestyle portrait animation framework. Enjoy <a href="https://github.com/mayuelala"> FollowYourEmoji</a> quickly !
	</li>
	<li>
		[03/2024] Excited to release <i style="color: red; display: inline;">Follow-Your-Click</i>, the first framework to achieve regional image animation. Enjoy the fun of <a href="https://github.com/mayuelala/FollowYourClick">FollowYourClick</a> !
	</li>
	<li>
		[01/2024] One paper are accepted by <a href="https://aaai.org/aaai-conference/">CVPR 2024</a> !  
	</li>
	<li>
		[12/2023] Two paper are accepted by <a href="https://aaai.org/aaai-conference/">AAAI 2024</a> !  
	</li>
	<li>
		[11/2023]  Excited to release <i style="color: red; display: inline;">MagicStick</i>, the first unified framework to modify video properties leveraging the keyframe transformations on the extracted internal control signals.  
	</li>
	<li>
		[10/2023] One paper is accepted by <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=6046">TMM 2024</a> !
	</li>
	<li>
		[09/2023] I am serving as a Reviewer for  <a href="https://iclr.cc/">ICLR 2024</a> !
	</li>
	<li>
		[07/2023] I start to cooperate with <a href="https://scholar.google.com/citations?user=lLMX9hcAAAAJ">Qifeng Chen</a> closely in <a href="https://cse.hkust.edu.hk/">HKUST</a> !
	</li>
	<li>
		[06/2023] I end my research intership in <a href="https://ai.tencent.com/">Tencent AI Lab</a> !
	</li>
	<li>
		[06/2023] I am serving as a Reviewer for  <a href="https://nips.cc/">NeurIPS 2023</a> !
	</li>
	<li>
		[05/2023] I am serving as a Reviewer for  <a href="https://www.acmmm2023.org/">ACM MM 2023</a> !
	</li>
	<li>
		[04/2023] Excited to release <i style="color: red; display: inline;">Follow-Your-Pose</i> framework, generating the character videos from pose and text description. Enjoy the fun of <a href="https://github.com/mayuelala/FollowYourPose">FollowYourPose</a> !
	</li>
	<li>
		[03/2023] Two paper are submitted to <a href="https://iccv2023.thecvf.com/">ICCV 2023</a> !
	</li>
	<li>
		[02/2023] One paper <a herf="https://arxiv.org/abs/2302.05940">SemanticAC</a> about Audio Classification was accepted by <a herf="https://2023.ieeeicassp.org/">ICASSP 2023</a> !
	</li>
	<li>
		[12/2022] Awarded First-Class Scholarship of <a herf="https://www.tsinghua.edu.cn/">Tsinghua Shenzhen International Graduate School</a> !
	</li>
	<li>
		[11/2022] One paper about video-text pretraining with masked autoencoder is submitted to  <a href="https://cvpr2023.thecvf.com/CVPR">CVPR 2023</a> !
	</li>
	<li>
		[10/2022] One paper about semantics-assisted framework for audio classification is submitted to <a href="https://2023.ieeeicassp.org/">ICASSP 2023</a> !
	</li>
	<li>
		[09/2022] Attending <a href="https://2022.acmmm.org/">ACM MM 2022</a>, Welcome to chat!
	</li>
	<li>
		[09/2022] <a href="https://">My first paper</a> has been accepted for <i style="color: red; display: inline;">Oral Presentation</i> on ACM MM 2022 (Top 5%) !
	</li>
	<li>
		[05/2022] Joined <a href="https://ai.tencent.com/">Tencent AI Lab</a> as research intern !
	</li>
	<li>
		[03/2022] Awarded <a href="https://www.withzz.com/project/detail/99">Tencent Rhino-Bird Research Elite Program</a> from Tencent ! Only <i style="color: red; display: inline;">72</i> students in the world admitted to this program !
	</li>
	<li>
		[07/2021] Joined <a href="http://mmlab.siat.ac.cn/">CUHK@MMLab in ShenZhen</a> as research intern !
	</li>
	<li>
		[06/2021] Recommended to <a href="https://www.tsinghua.edu.cn/">Tsinghua University</a> towards the MSc degree !
	</li>
	<li>
		[06/2021] Graduated from <a href="http://ccst.tyut.edu.cn/">Taiyuan University of Technology</a> !
	</li>
	<li>
		[01/2021] Joined <a href="https://www.tsinghua.edu.cn/">Baidu Research</a> as research intern. Started doing research on Computer Vision !
	</li>
</ul>
</div>

<h2>Industrial Experience</h2>
<table id="tbPublications" width="100%">
	<tbody>
	<p></p>
	</td>
	<tr>
	<tr>
		<td width="306">
		<img src="./industrial/meta.png" width="240px" style="box-shadow: 4px 4px 8px #ffffff">
		</td>				
		<td>
		<p><b> Meta Reality Lab</b></p>
		<p>June. 2025 - Dec. 2025, Meta Reality Lab </p>
		<em>Topic: 3D/4D Generation and World Model</em>
		</td>
	</tr>
</tr>
<tr>&nbsp</tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
	<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
	<tr>
	<tr>
		<td width="306">
		<img src="./industrial/hunyuan.png" width="255px" style="box-shadow: 4px 4px 8px #ffffff">
		</td>				
		<td>
		<p><b> Tencent Hunyuan</b></p>
		<p>Dec. 2023 - Dec.2024, Tencent Hunyuan, ShenZhen, China </p>
		<em>Topic: Video Generation (Follow Family) and Video Editing</em>
		</td>
	</tr>
	<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
	<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
	<tr>
		<td width="306">
		<img src="./industrial/tencent.png" width="255x" style="box-shadow: 4px 4px 8px #ffffff">
		</td>				
		<td>
		<p><b> Tencent AI Lab</b></p>
		<p>May. 2022 - July. 2023, Tencent AI Lab, Visual Center, ShenZhen, China </p>
		<p>worked with <a href="https://tianyu-yang.com/" target="_blank">Dr. Tianyu Yang</a>, <a href="https://vinthony.github.io/" target="_blank">Dr. Xiaodong Cun</a> and <a href="https://xinntao.github.io/" target="_blank">Dr. Xintao Wang</a> </p>
		<em>Topic: Video Generation and Video Editing</em>
		</td>
	</tr>
	<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
	<!--########################-->
	</td>
	<tr>
	<tr>
	<tr>
	<tr>
	<tr>
	<tr>
		<td width="306">
		<img src="./industrial/baidu.png" width="255px" style="box-shadow: 4px 4px 8px #ffffff">
		</td>				
		<td>
		<p><b> Baidu VIS</b></p>
		<p>Jan. 2021 - July. 2021, Baidu, Visual group, ShenZhen, China </p>
		<p>worked with Zhikang Zou </p>
		<em>Topic: 3D Object Detection, 3D Image Understanding</em>
		</td>
	</tr>
	<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
	<!--########################-->
</tbody></table>


<!-- The University of Sydney, Australia

Doctor of Philosophy (Ph.D.) Candidate in Computer Science
Team: Multimedia Laboratory, Sydney (MMLab@Sydney)
 Prof. Wanli Ouyang, Prof. Chang Xu
2022 - Present -->



<h2>Education & Visiting</h2>
<table id="tbPublications" width="100%">
	<tbody>
		<td width="306">
		<img src="./education/hkust.png" width="270px" style="box-shadow: 4px 4px 8px #ffffff">
		</td>				
		<td>
		<p><b>The Hong Kong University of Science and Technology, Hong Kong</b></p>
		<p>PhD Student in Visual Intelligence Lab, HKUST </p>
		<p>Advisor: <a href="https://cqf.io/">Prof. Qifeng Chen</a> </p>
		<p>Sep. 2024 - Future <p>
		</p>
		</td>
	</tr>
	<!--########################-->
	<tbody>
		<td width="306">
		<img src="./education/hkust.png" width="270px" style="box-shadow: 4px 4px 8px #ffffff">
		</td>				
		<td>
		<p><b>The Hong Kong University of Science and Technology, Hong Kong</b></p>
		<p>Research Assistant in Visual Intelligence Lab, HKUST </p>
		<p>Advisor: <a href="https://cqf.io/">Prof. Qifeng Chen</a> </p>
		<p>July. 2023 - Nov. 2023 <p>
		</p>
		</td>
	</tr>
	<!--########################-->
	<p></p>
	</td>
	<tr>
	<!-- <tr>&nbsp</tr>
	<tr>&nbsp</tr> -->
	<tr>
		<td width="306">
		<img src="./education/thu.png" width="210px" style="box-shadow: 4px 4px 8px #ffffff" >	
		</td>
			
		<td>
		<p><b>Tsinghua University, China</b></p>
		<p>Master of Engineering in Electronic Information </p>
		<p>Advisor: <a href="https://scholar.google.com/citations?user=Xrh1OIUAAAAJ&hl=zh-CN">Prof.Xiu Li</a></p>
		<p>Sep. 2021 - Jun. 2024 <p>
		</p>
		</td>
	</tr>

	<!--########################-->
		<p></p>
	</td>
	<tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr>
	<tr>
		<td width="306">
		<img src="./education/siat.png" width="285px" style="box-shadow: 4px 4px 8px #ffffff">
		</td>				
		<td>
		<p><b> University of Chinese Academy of Sciences, China</b></p>
		<p>Research Assistant in Multimedia Laboratory, SIAT, CAS (<a href="https://mmlab.siat.ac.cn/">MMLab@SIAT</a>) </p>
		<p>Advisor: <a href="https://scholar.google.com.hk/citations?hl=zh-CN&user=gFtI-8QAAAAJ&view_op=list_works&sortby=pubdate">Prof. Yu Qiao</a> and  <a href="https://scholar.google.com.hk/citations?user=hD948dkAAAAJ&hl=zh-CN">Dr. Yali Wang</a></p>
		<p>Jul. 2021 - Apr. 2022 <p>
		</p>
		</td>
	</tr>
	<!--########################-->
	<tr>
		<tr>
			<td width="306">
			<img src="./education/tyut.png" width="270px" style="box-shadow: 4px 4px 8px #ffffff">
			</td>				
			<td>
			<p><b>Taiyuan University of technology, China</b></p>
			<p>Bachelor of Engineering in Computer Science </p>
			<p>Sep. 2017 - Jun. 2021 <p>
			</p>
			</td>
		</tr>
		<tr>&nbsp</tr>

	<!--########################-->
		
</tbody></table>


<h2> Selected Publications | <a href="https://scholar.google.com/citations?user=kwBR1ygAAAAJ&hl=zh-CN">Full List</a></h2>
<!--
<div style="height: 1440px; overflow: auto;">
-->
<table id="tbPublications" width="100%">
	<tbody>
	<td><b>/*Preprints*/</b>
	</td>
	<tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr>
	<tr>
		<td width="306">
		<img src="./indexpics/2025-survey.png" width="285px" style="box-shadow: 4px 4px 8px #888">
		</td>				
		<td>
		<p><b>üéâ Controllable Video Generation: A Survey</b></p>
		<p><b>Yue Ma</b>, Kunyu Feng, Zhongyuan Hu, Xinyu Wang, Yucheng Wang, Mingzhe Zheng, Xuanhua He, Chenyang Zhu, Hongyu Liu, Yingqing He, Zeyu Wang, Zhifeng Li, Xiu Li, Wei Liu, Dan Xu, Linfeng Zhang, Qifeng Chen</p>
		<em>arXiv preprint:2507.16869. 2025</em>
		<p> [<a href="https://arxiv.org/pdf/2507.16869">paper</a>] [<a href="https://github.com/mayuelala/Awesome-Controllable-Video-Generation">code</a>] </p>
		</td>
	</tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr>
	<tr>
		<td width="306">
		<img src="./indexpics/2025-nips.png" width="285px" style="box-shadow: 4px 4px 8px #888">
		</td>				
		<td>
		<p><b>üéâ Follow-Your-Creation: Empowering 4D Creation through Video Inpainting</b></p>
		<p><b>Yue Ma</b>, Kunyu Feng, Xinhua Zhang,   Hongyu Liu, David Junhao Zhang,   Jinbo Xing,   Yinhan Zhang,   Ayden Yang,  Zeyu Wang,  Qifeng Chen</p>
		<em>arXiv preprint:2506.04590. 2025</em>
		<p> [<a href="https://arxiv.org/pdf/2506.04590">paper</a>] [<a href="https://follow-your-creation.github.io/">code</a>] </p>
		</td>
	</tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr>
	<tr>
		<td width="306">
		<img src="./indexpics/2025-motion.png" width="285px" style="box-shadow: 4px 4px 8px #888">
		</td>				
		<td>
		<p><b>üéâ Follow-Your-Motion: Video Motion Transfer via Efficient Spatial-Temporal Decoupled Finetuning</b></p>
		<p><b>Yue Ma</b>, Yulong Liu, Qiyuan Zhu, Ayden Yang, Kunyu Feng, Xinhua Zhang, Zhifeng Li, Sirui Han, Chenyang Qi, Qifeng Chen</p>
		<em>arXiv preprint:2506.05207. 2025</em>
		<p> [<a href="https://arxiv.org/pdf/2506.05207">paper</a>] [<a href="https://follow-your-motion.github.io/">code</a>] </p>
		</td>
	</tr>
	<!--########################-->
	<!-- <tr>
		<td width="306">
		<img src="./indexpics/2022-simvtp-framework.png" width="285px" style="box-shadow: 4px 4px 8px #888">
		</td>				
		<td>
		<p><b>SimVTP: Simple Video Text Pre-training with Masked Autoencoders</b></p>
		<p><b>Yue Ma</b>, Tianyu Yang, Ying Shan, Xiu Li</p>
		<em>arXiv preprint:2211.03490. 2022</em>
		<p> [<a href="https://arxiv.org/pdf/2211.03490">paper</a>] [<a href="https://github.com/mayuelala/SimVTP">code</a>] </p>
		</td>
	</tr> -->
	<!--########################-->
	<tr>&nbsp</tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr>
	<td><b>/*Journal*/</b>
	
	<tr>	
	<td width="306">
	<img src="./indexpics/2025-IJCV.png" width="285px" style="box-shadow: 4px 4px 8px #888">
	</td>				
	<td>
	<p><b>Follow-Your-Emoji-Faster: Towards Efficient, Fine-Controllable, and Expressive Freestyle Portrait Animation</b></p>
	<p><b>Yue Ma</b>, Zexuan Yan, Hongyu Liu, Hongfa Wang, Heng Pan, Yingqing He, Junkun Yuan, Ailing Zeng, Chengfei Cai, Heung-Yeung Shum, Zhifeng Li, Wei Liu, Linfeng Zhang, Qifeng Chen</p>
	<em>International Journal of Computer Vision(<b>IJCV</b>), 2025</em>
	<p> [<a href="https://mayuelala.github.io/">paper</a>] [<a href="https://github.com/mayuelala/FollowYourClick">code</a>] [<a href="https://follow-your-click.github.io/">project page</a>] 
	</p>
	</td>
	</tr>
	<td><b>/*Conference*/</b>
	<p></p>
	</td>
	<tr>	
	<td width="306">
	<img src="./indexpics/2024-followyourclick.png" width="285px" style="box-shadow: 4px 4px 8px #888">
	</td>				
	<td>
	<p><b>Follow-Your-Click: Open-domain Regional Image Animation via Short Prompts</b></p>
	<p><b>Yue Ma</b>, Yingqing He, Hongfa Wang, Andong Wang, Chenyang Qi, Chengfei Cai, Xiu Li, Zhifeng Li, Heung-Yeung Shum, Wei Liu, Qifeng Chen</p>
	<em>The 39th Annual AAAI Conference on Artificial Intelligence (<b>AAAI</b>), 2025</em>
	<p> [<a href="https://arxiv.org/abs/2403.08268">paper</a>] [<a href="https://github.com/mayuelala/FollowYourClick">code</a>] [<a href="https://follow-your-click.github.io/">project page</a>] 
	</p>
	</td>
	</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
	<tr>
		<td width="306">
		<img src="./indexpics/2024-Canvas.png" width="285px" style="box-shadow: 4px 4px 8px #888">
		</td>				
		<td>
		<p><b>Follow-Your-Canvas: Higher-Resolution Video Outpainting with Extensive Content Generation</b></p>
		<p>Qihua Chen*, <b>Yue Ma*</b>, Hongfa Wang*, Junkun Yuan*, Wenzhe Zhao, Qi Tian, Hongmei Wang, Shaobo Min, Qifeng Chen, Wei Liu</p>
		<em>The 39th Annual AAAI Conference on Artificial Intelligence (<b>AAAI</b>), 2025</em>
		<p> [<a href="hhttps://arxiv.org/abs/2409.01055">paper</a>] [<a href="https://github.com/mayuelala">code</a>] [<a href="https://github.com/mayuelala">project page</a>] 
		</p>
		</td>
	</tr>
	<tr>&nbsp</tr>

		
	<!--########################-->
	<tr>
		<td width="306">
		<img src="./indexpics/2024-Fypv2.png" width="285px" style="box-shadow: 4px 4px 8px #888">
		</td>				
		<td>
		<p><b>Follow-Your-Pose v2: Multiple-Condition Guided Character Image Animation for Stable Pose Control</b></p>
		<p>Jingyun Xue, Hongfa Wang, Qi Tian, <b>Yue Ma</b>, Andong Wang, Zhiyuan Zhao, Shaobo Min, Wenzhe Zhao, Kaihao Zhang, Heung-Yeung Shum, Wei Liu, Mengyang Liu, Wenhan Luo</p>
		<em>International Conference on Learning Representations(<b>ICLR</b>) 2025</em>
		<p> [<a href="hhttps://arxiv.org/abs/2406.03035">paper</a>] [<a href="https://github.com/mayuelala">code</a>] [<a href="https://github.com/mayuelala">project page</a>] 
		</p>
		</td>
	</tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr>
	<!--########################-->
	<tr></tr>
		<td width="306">
		<img src="./indexpics/2024-followyouremoji.png" width="285px" style="box-shadow: 4px 4px 8px #888">
		</td>				
		<td>
		<p><b>Follow-Your-Emoji: Fine-Controllable and Expressive Freestyle Portrait Animation</b></p>
		<p><b>Yue Ma</b>, Hongyu Liu, Hongfa Wang, Heng Pan, Yingqing He, Junkun Yuan, Ailing Zeng, Chengfei Cai, Heung-Yeung Shum, Wei Liu, Qifeng Chen</p>
		<em>The ACM Special Interest Group for Computer Graphics and Interactive Techniques(<b>Siggraph Asia</b>) 2024</em>
		<p> [<a href="https://arxiv.org/abs/2406.01900">paper</a>] [<a href="https://github.com/mayuelala">code</a>] [<a href="https://follow-your-emoji.github.io/">project page</a>] 
		</p>
		</td>
	</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
	<tr>
		<td width="306">
		<img src="./indexpics/2023-MagicStick.png" width="285px" style="box-shadow: 4px 4px 8px #888">
		</td>				
		<td>
		<p><b>Follow-Your-Handle: Controllable Video Editing via Control Handle Transformations</b></p>
		<p><b>Yue Ma</b>, Xiaodong Cun, Yingqing He, Chenyang Qi, Xintao Wang, Ying Shan, Xiu Li, Qifeng Chen</p>
		<em>IEEE /CVF Winter Conference on Applications of Computer Vision (<b>WACV</b>), 2025</em>
		<p> [<a href="https://arxiv.org/abs/2312.03047">paper</a>] [<a href="https://github.com/mayuelala/MagicStick">code</a>] [<a href="https://magic-stick-edit.github.io/">project page</a>] 
		</p>
		</td>
	</tr>
	<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
	<!--########################-->
	<tr>
		<td width="306">
		<img src="./indexpics/2023-iccv-followyourpose.png" width="285px" style="box-shadow: 4px 4px 8px #888">
		</td>				
		<td>
		<p><b>üï∫üï∫üï∫ Follow-Your-Pose üíÉüíÉüíÉ: Pose-Guided Text-to-Video Generation using Pose-Free Videos</b></p>
		<p><b>Yue Ma</b>, Yingqing He, Xiaodong Cun, Xintao Wang, Siran Chen, Ying Shan, Xiu Li, Qifeng Chen</p>
		<em>The 38th Annual AAAI Conference on Artificial Intelligence (<b>AAAI</b>), 2024</em>
		<p><a href="https://www.paperdigest.org/2024/09/most-influential-aaai-papers-2024-09/" style="color: red; display: inline;"><b>PaperDigest Most Influential Papers of AAAI 24</b></a></p>
		<p> [<a href="https://arxiv.org/abs/2304.01186">paper</a>] [<a href="https://github.com/mayuelala/FollowYourPose">code</a>] [<a href="https://follow-your-pose.github.io/">project page</a>] 
		<a target="_blank" href ="https://github.com/mayuelala/FollowYourPose"><img alt="GitHub stars" align="right" src="https://img.shields.io/github/stars/mayuelala/FollowYourPose?style=social"></a><a></a></p>
		</td>
	</tr>
	<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
	<!--########################-->
	<!-- <tr>
		<td width="306">
		<img src="./indexpics/2024-AAAI-MBEV.png" width="285px" style="box-shadow: 4px 4px 8px #888">
		</td>				
		<td>
		<p><b>M-BEV: Masked BEV Perception for Robust Autonomous Driving</b></p>
		<p>Siran Chen, <b>Yue Ma</b>, Yu Qiao, Yali Wang</p>
		<em>The 38th Annual AAAI Conference on Artificial Intelligence (<b>AAAI</b>), 2024</em>
		<p> [<a href="https://arxiv.org/abs/2304.01186">paper</a>] [<a href="https://github.com/mayuelala/FollowYourPose">code</a>] [<a href="https://follow-your-pose.github.io/">project page</a>] 
		</td>
	</tr>
	<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr>
		<tr>&nbsp</tr> -->
	<!-- <tr>
		<td width="306">
		<img src="./indexpics/2023-icassp-audio.png" width="285px" style="box-shadow: 4px 4px 8px #888">
		</td>				
		<td>
		<p><b>SemanticAC: Semantics-Assisted Framework for Audio Classification</b></p>
		<p>Yicheng Xiao*, <b>Yue Ma*</b>, Shuyan Li, Hantao Zhou, Ran Liao, Xiu Li (* equal contribution)</p>
		<em>IEEE International Conference on Acoustics, Speech and Signal Processing (<b>ICASSP</b>), 2023. </em>
		<i></i>
		<p> [<a href="https://arxiv.org/abs/2302.05940">paper</a>] [<a href="https://github.com/mayuelala">code</a>] </p>
		</td>
	</tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr> -->
	<tr>
		<td width="306">
		<img src="./indexpics/2022-mm-graph.png" width="285px" style="box-shadow: 4px 4px 8px #888">
		</td>				
		<td>
		<p><b>Visual Knowledge Graph for Human Action Reasoning in Videos</b></p>
		<p><b>Yue Ma</b>, Yali Wang, Yue Wu, Ziyu Lyu, Siran Chen, Xiu Li, Yu Qiao</p>
		<em>The 30th ACM International Conference on Multimedia. (<b>ACM MM</b>), 2022. </em>
		<i><p style="color: red; display: inline;">(<b>Oral Presentation</b>)</p></i>
		<p> [<a href="https://dl.acm.org/doi/abs/10.1145/3503161.3548257">paper</a>] [<a href="https://github.com/mayuelala/AKU">code</a>] </p>
		</td>
	</tr>
	<tr>&nbsp</tr>
    	<tr>&nbsp</tr>
    	<tr>&nbsp</tr>
		
	<!-- <td><b>/*Journal*/</b>
		<p></p>
		</td>
		
		<tr>
			<td width="306">
			<img src="./indexpics/2023-TMM-mmlab.png" width="285px" style="box-shadow: 4px 4px 8px #888">
			</td>				
			<td>
			<p><b>Attentive Snippet Prompting for Video Retrieval</b></p>
			<p>Siran Chen, Qinglin Xu, <b>Yue Ma</b>, Yu Qiao, Yali Wang</p>
			<em>IEEE Transactions on Multimedia (<b>TMM</b>), 2024. </em>
			<i></i>
			<p> [<a href="https://ieeexplore.ieee.org/abstract/document/10268993/">paper</a>] [<a href="https://ieeexplore.ieee.org/abstract/document/10268993/">code</a>] </p>
			</td>
		</tr> -->
		<tr>&nbsp</tr>
			<tr>&nbsp</tr>
			<tr>&nbsp</tr>
			<tr>&nbsp</tr>
			<tr>&nbsp</tr>
	

</tbody></table>
<!--
</div>
-->




<h2>Honors &amp; Awards</h2>
<table style="border-spacing:2px">
	<tbody>
	<tr><td> [06/2024] Outstanding graduates student of Beijing.</td></tr>
	<tr><td> [08/2023] First-Class Scholarship of <a herf="https://www.tsinghua.edu.cn/">Tsinghua University</a>.</td></tr>
	<tr><td> [12/2022] First-Class Scholarship of <a herf="https://www.tsinghua.edu.cn/">SIGS</a>, Tsinghua University.</td></tr>
	<tr><td> [03/2022] <a href="https://www.withzz.com/project/detail/99">Tencent Rhino-Bird Research Elite Program</a>, only 72 students in the world admitted to this program.</td></tr>
	<tr><td> [09/2020] Scholarship for Academic Excellence of <a href="http://ccst.tyut.edu.cn/">Taiyuan University of Technology</a>.</td></tr>.
	<tr><td> [06/2019] Excellent Scientific Student of <a href="http://ccst.tyut.edu.cn/">Taiyuan University of Technology</a>.</td></tr>.
	<tr><td> [09/2019] Scholarship for Academic Excellence of <a href="http://ccst.tyut.edu.cn/">Taiyuan University of Technology</a>.</td></tr>
	<tr><td> [09/2018] Excellent Academic Progress Student of <a href="http://ccst.tyut.edu.cn/">Taiyuan University of Technology</a>.</td></tr>.
	<tr><td> [06/2018] Scholarship for Academic Excellence of <a href="http://ccst.tyut.edu.cn/">Taiyuan University of Technology</a>.</td></tr>.

	</tbody>
</table>


<h2>Professional Services</h2>
<ul>
	<li>	
	<b>Student Reviewers:</b><br>
	Computer Vision and Pattern Recognition (CVPR)<br>
	International Conference on Computer Vision (ICCV)<br>
	European Conference on Computer Vision (ECCV)<br>
	Conference and Workshop on Neural Information Processing Systems (NeurIPS)<br>
	International Conference on Learning Representations (ICLR) <br>
	AAAI Conference on Artificial Intelligence (AAAI)<br>
	IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)
	</li>


</ul>


<h2>Teaching</h2>
<table id="tbTeaching" border="0" width="100%">
	<tbody>
		<tr>
			<td> 2022-2023</td><td>Fall</td><td>Artificial Intelligence Technology (THU, 85990263-200)</td>
		</tr>
	</tbody>
</table>




<div id="footer">
	<div id="footer-text"></div>
</div>
	<script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=777777&w=487&t=tt&d=xeJu_Kwek6AfO5eDCKFQ1iDWjzFQPLT_dNcYY3WLmrY&co=ffffff&cmo=3acc3a&cmn=ff5353&ct=5b1717'></script>
	<p><center> &copy; Jack Ma </center></p>


</div>
</body></html>



